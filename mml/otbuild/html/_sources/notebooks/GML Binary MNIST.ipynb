{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Ground Metric Learning on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of the notebook is to find the ground metric of the Wasserstein distance using 2 true classes as the input, i.e.: \n",
    "- For MNIST we use 30 examples (ideally is to have balanced classes) of each class and transform the problem of  binary classification into finding 2 clusters that are close in wasserstein distance\n",
    "- Same for Caltech 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "* mml.gml - Wasserstein distances\n",
    "* mml.gml - OT matrices\n",
    "* mml.gml - Similarity matrix\n",
    "* mml.datasets - Datasets loading\n",
    "\n",
    "** ALGORITHM 1 and Algorithm 2 are implemented ** \n",
    "\n",
    "When using the off-diagonal ones as a ground matrix, this puts all histograms at distance 1. This has two drawbacks, from Cuturi's paper: \n",
    "    * uninformative gradient in the first iterations\n",
    "    * far from the optimum so will converge slowly.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm_notebook\n",
    "from tqdm import trange\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Your code goes here\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from numpy import testing\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "import metric_learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.neighbors.ball_tree import BallTree\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Shogun - Metric Learning\n",
    "from shogun import LMNN as shogun_LMNN\n",
    "from shogun import RealFeatures, MulticlassLabels\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "# POT imports\n",
    "import ot\n",
    "from ot.datasets import get_1D_gauss as gauss\n",
    "\n",
    "# MML import \n",
    "from mml import wasserstein, transform, gml, ot_testing, datasets, helper\n",
    "\n",
    "data_path = str(Path(os.getcwd())) + \"/data/\"\n",
    "results_path = str(Path(os.getcwd())) + \"/results/binary\"\n",
    "\n",
    "def write_to_pickle(dataframe, name):\n",
    "    dataframe.to_pickle(data_path + name + \".pickle\")\n",
    "def read_from_pickle(name): \n",
    "    return pd.read_pickle(data_path + name + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(wasserstein)\n",
    "importlib.reload(gml)\n",
    "importlib.reload(helper)\n",
    "# Load Caltech Data from disk \n",
    "#X_caltech = np.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select from labels pairs of datapoints to construct multiple training sets for GML. \n",
    "* Below we select binary classification for digit 1 and digit 2 or for the caltech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 45, 42, 51, 38, 51, 43, 44, 51, 52])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Hellinger representation of the data \n",
    "# TODO: Change method in load_mnist to first look on disk before anything else \n",
    "X,Y = datasets.load_mnist(\"Hellinger\")\n",
    "train_size=0.25\n",
    "test_size=0.75\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=train_size,test_size=test_size,random_state=123)\n",
    "np.bincount(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have at most 45 unique pairs for MNIST, pick two classes\n",
    "# For Caltech, classes are balanced\n",
    "data_dict = datasets.dictionary(X,Y)\n",
    "\n",
    "class1 = 1\n",
    "class2 = 2 \n",
    "[x12,y12] = datasets.data_pairs(data_dict,class1,class2)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(x12,y12,train_size=train_size,test_size=test_size,random_state=123)\n",
    "\n",
    "# Transform into histogram \n",
    "# Neither Wasserstein POT nor LMNN are normalized, have to do it yourself\n",
    "#[X_train_normalized, X_test_normalized] = transform.normalize(X_train,X_test,'l1')\n",
    "\n",
    "X_train_normalized = X_train/X_train.sum(axis=1).reshape((-1,1))\n",
    "X_test_normalized = X_test/X_test.sum(axis=1).reshape((-1,1))\n",
    "n = X_train_normalized.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are independent of what data we choose, only on the dimensionality of a datapoint, i.e. here is 64\n",
    "d = X_train.shape[1]\n",
    "x = np.arange(d,dtype=np.float64)\n",
    "x1 = x.reshape((d,1))\n",
    "# By default metric ='sqeuclidean' in the function\n",
    "M_sqeuclidean = ot.dist(x1,x1,metric='sqeuclidean')\n",
    "M_eye = ot.dist(x1,x1,metric='hamming')\n",
    "\n",
    "xx,yy = np.meshgrid(np.arange(np.sqrt(d)),np.arange(np.sqrt(d)))\n",
    "xy = np.hstack((xx.reshape(-1,1),yy.reshape(-1,1)))\n",
    "M_mesh = ot.dist(xy, xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GML $M^*$ and $W_d$ via Cuturi \n",
    "Questions to answer\n",
    "- why the seesaw objective? \n",
    "- are digits normalized? no, we need to perform that. does POT do the normalization step itself? NO, we need to do it ourselves\n",
    "- We need to build $M_1$, the initial distance between digits for our algorithm. Follow the algo for doing this.\n",
    "- Try Wasserstein with Euclidean distance so that you have an idea of the accuracy. Errors are important too though. \n",
    "- Randomize the whole training so you can get some error bars. \n",
    "Step 1: Find $D(digit1,digit2)$ as Euclidean and an input to the Wasserstein algorithm. \n",
    "Step 2: Have a way of computing $X^*$ for two digits. This is the OT distance. (have another one using Sinkhorn)\n",
    "Step 3: 1)M and 2)X will be 64x64 matrices with each entry comparing points in 1)data space 2) probability space\n",
    "\n",
    "Learning ground metrics. \n",
    "- Where does the similarity matrix come from? i.e. for us it's multiclass. Build $w_ij$ for similar histograms using 1/nk\n",
    "- For the feasible set of ground metrics, M has to have L1 norm of 1. \n",
    "- Find $W(r_i,r_j)$ such that it's small for similar images, and big for dissimilar images. Problem is all dissimilarities are on the same page - which is not entirely true. Can we do better if we engineer that? \n",
    "- Remember, Wasserstein-2 is $G_i,j(M) = <X^*,M>$ and gradient of that is $X^*$. You need to use these. \n",
    "- M is only one. that doesn't sound right...\n",
    "- How do you compute the similar/dissimilar neighbours. You need to keep track of 1437x1437 wasserstein distances. Then you need to find the k closest neighbours. Or maybe half of that. Each line has all neighbours of the first position in the line. But what about when i=4 and you're only looking at j > 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build similarity matrix:**\n",
    "* k_neigh = 3 \n",
    "* Can you build $w$ differently, i.e. by encoding some prior information that tells you how close r_i and r_j are? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neigh = 3\n",
    "w = gml.similarity_matrix(X_train_normalized,Y_train,k_neigh)\n",
    "plt.imshow(w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_file = \"matrix12\"\n",
    "gml_logger = None\n",
    "M_learned, objective_tracker, product_tracker, interval_tracker = gml.projected_subgradient(\n",
    "                                X_train_normalized, M_mesh ,w, matrix_file, gml_logger, p_max = 2, q_max = 3, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.save_ndarray(\"results_binary/gml12_mesh_vectorized\",M_learned)\n",
    "plt.imshow(M_learned);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper.save_json(results_path+\"gml12_objective\",objective_tracker)\n",
    "plt.plot(objective_tracker);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data  = helper.load_json(results_path+\"gml12_objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(interval_tracker);\n",
    "plt.plot(product_tracker);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below not used, only for debugging purposes\n",
    "**Cuturi's approach** \n",
    "* q_max is set to 80 and the inner loop is run at minimum 24 times\n",
    "* Other criteria can be change in objective does not change more than 0.75% every 8 steps\n",
    "    - I need to do this and plot the learning for z_plus and z_threshold. \n",
    "    - First do z_threshold (or z_in)\n",
    "    - where the hell did they get this? \n",
    "* A maximum of 20 outer loops - so p_max = 20\n",
    "* What does the t step do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def projected_subgradient(X_train_normalized, M_original_eye, w, p_max, q_max, early_stopping=False):\n",
    "    X = X_train_normalized\n",
    "    # M_0 = total_variation()\n",
    "    M_0 = M_original_eye/(X.shape[0]*(X.shape[0]-1))\n",
    "    t = 1 \n",
    "    p = 0\n",
    "    learning_rate = 0.1\n",
    "    M_p_outer = M_0    \n",
    "\n",
    "    #with tqdm.tqdm(total=p_max) as outer_progress_bar:\n",
    "    #outer_progress_bar.set_description(\"Outer Loop Progress\")\n",
    "    pbar = tqdm_notebook(total = p_max)\n",
    "    pbar.set_description(\"Outer Loop progress\")\n",
    "    objective_tracker = []\n",
    "    product_tracker = []\n",
    "    interval_tracker = []\n",
    "    change_M = []\n",
    "    z_results = []\n",
    "    gamma = []\n",
    "    outer_M = []\n",
    "    \n",
    "    q_min = 24 \n",
    "    interval = 8\n",
    "    \n",
    "    while p < p_max: \n",
    "        #print(\"Outer Loop: Iteration {0}\".format(p))\n",
    "        G, T = gml.compute_distances(X_train_normalized, M_p_outer, showProgress=False)\n",
    "        #outer_M.append(M_p_outer)\n",
    "        z_plus, gamma_plus = gml.algorithm1_similar(M_p_outer, w, G, T, 3)\n",
    "        q = 0 \n",
    "        M_q_inner = M_p_outer\n",
    "        #while (z_threshold < 0.001) - capture the change in the objective\n",
    "        #with tqdm.tqdm(total=q_max) as inner_progress_bar:\n",
    "        #inner_progress_bar.set_description(\"Inner Loop Progress\")\n",
    "        qbar = tqdm_notebook(total=q_max)\n",
    "        qbar.set_description(\"Inner Loop progress\")\n",
    "        while q < q_max:\n",
    "            #print(\"Inner Loop: Iteration {0}\".format(q))\n",
    "            G, T = gml.compute_distances(X_train_normalized, M_q_inner, showProgress=False)\n",
    "            z_minus, gamma_minus = gml.algorithm1_dissimilar(M_q_inner, w, G, T, 3)\n",
    "            # The following line should be a pure number, the criteria/error where optimizing\n",
    "            # Please make sure the res value is correctly computed, as the algo doesn't say much. \n",
    "            # you should deduce from the paper whether it's a scalar or not, although it should be. \n",
    "            # Maybe the transpose was his way of saying this is a sum...\n",
    "            # Maybe you don't actually have to do any transpose for gamma plus ...\n",
    "            diff = helper.tril_vector(M_q_inner) - helper.tril_vector(M_p_outer)\n",
    "            # Also need to see how this diff evolves. why does the spike appear!!!!!\n",
    "            \n",
    "            # Basically it starts with previous value somehow.. \n",
    "            res = gamma_plus.T.dot(diff)\n",
    "            product_tracker.append(res)\n",
    "            z_results.append(z_minus + z_plus)\n",
    "            \n",
    "            # This is the final objective list\n",
    "            z_threshold = z_minus + z_plus + res\n",
    "            objective_tracker.append(z_threshold)\n",
    "            \n",
    "            # Gradient descent \n",
    "            #wasserstein.lower_triangular(M_q_inner)\n",
    "            #gamma.append(gamma_plus+gamma_minus)\n",
    "            gamma.append(G)\n",
    "            M_q_inner_lower = helper.tril_vector(M_q_inner) - (learning_rate/np.sqrt(q+1)) * (gamma_plus + gamma_minus)\n",
    "            change_M.append(M_q_inner_lower)\n",
    "            \n",
    "            # UnComment here for full symmetric metric matrix\n",
    "            M_q_inner = helper.symmetrize_from_vector(M_q_inner_lower,M_0.shape[0])\n",
    "            \n",
    "            # Below is only lower diagonal, I don't know in what world this projection would work\n",
    "            #M_q_inner = wasserstein.matrix_from_vector(M_q_inner_lower,M_0.shape[0])\n",
    "                \n",
    "            # Because the output of this algorithm might not be a metric, \n",
    "            # use metric nearness to bring it closer to its real properties.\n",
    "            M_q_inner= gml.project_metric(M_q_inner)    \n",
    "        \n",
    "            q += 1\n",
    "            t +=1\n",
    "            #inner_progress_bar.update(1)    \n",
    "            qbar.update(1)\n",
    "            \n",
    "            # Early Stopping\n",
    "            # Stop the inner loop if between 8 steps it didn't progress more than 0.75% \n",
    "            if early_stopping and q > q_min: \n",
    "                previous_thres = objective_tracker[-interval]\n",
    "                val = (z_threshold - previous_thres)/previous_thres * 100\n",
    "                interval_tracker.append(np.abs(val))\n",
    "                if np.abs(val) < 0.75:\n",
    "                    break;\n",
    "\n",
    "            \n",
    "        print(\"assign M inner to outer\")\n",
    "        M_p_outer = M_q_inner\n",
    "        p +=1\n",
    "        #outer_progress_bar.update(1)\n",
    "        pbar.update(1)\n",
    "    return M_p_outer, product_tracker, objective_tracker, interval_tracker, change_M, z_results, gamma, outer_M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just to show that there are numerical errors when computing the coupling/distances. \n",
    "# In the algorithms, I just say entry at i,j equals j,i, so T1 == T2 \n",
    "r = X_train_normalized[1]\n",
    "c = X_train_normalized[3]\n",
    "m = X_train_normalized[2]\n",
    "n = X_train_normalized[4]\n",
    "\n",
    "G1 = wasserstein.distance(r,c, **{'ground metric': M_original_eye})\n",
    "[T1,M1] = wasserstein.coupling(r,c,**{'ground metric': M_original_eye})\n",
    "\n",
    "[T2,M2] = wasserstein.coupling(c,r,**{'ground metric': M_original_eye})\n",
    "\n",
    "\n",
    "print('Distance is: {0}'.format(G1))\n",
    "\n",
    "# Check that T1 is correct - why not if so? In my algo I take them as being equal as it should be\n",
    "rec_r = np.sum(T1, axis=1)\n",
    "rec_c = np.sum(T1,axis=0)\n",
    "\n",
    "# Check that distance is the same as the Frobenius product \n",
    "testing.assert_allclose(np.sum(T1 * M1),G1)\n",
    "\n",
    "# Should they not be symmetric?\n",
    "(np.transpose(T1) != T2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate all $W_2$ distances for the dataset as well as all $T$** \n",
    "Can it be optimized even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all = wasserstein.all_distances(X_train_normalized,M_eye)\n",
    "T_all = wasserstein.all_couplings(X_train_normalized,M_eye,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "n_proc = multiprocessing.cpu_count()\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check for some properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Equal to T2 since we've done lower diagonal computation\n",
    "(T_all[1,3] != T2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symmetry Test\n",
    "ot_testing.is_metric(G_all)\n",
    "\n",
    "# Triangle inequality\n",
    "print(G_all[1][2] + G_all[2][3] >= G_all[1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compose subsets of similar and dissimilar neighbours, everything below is in the algo1 method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find neighbours\n",
    "S_similar = gml.similar_neighbours(3,w,G_all)\n",
    "S_dissimilar = gml.dissimilar_neighbours(3,w,G_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more debugging that was written prior to the Testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_10_sum =  gml.sum_neighbours(S_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val = np.argsort(G_10[3,:],kind='mergesort')\n",
    "val1  = np.argsort(G_10[5,:],kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_10 = gml.similar_neighbours(3,w_10,G_10)\n",
    "grad_10 = gml.gradient_neighbours(w_10,T_10,S_10)\n",
    "grad_10[1][0][0].shape\n",
    "# --- In the method -- \n",
    "# helper.triu_vector(gamma_plus) + helper.tril_vector(gamma_plus)\n",
    "plt.imshow(helper.triu(grad_objective_plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = helper.triu(grad_objective_plus)\n",
    "a = helper.tril(grad_objective_plus)\n",
    "\n",
    "b = helper.tril(grad_objective_minus)\n",
    "b1 = helper.triu(grad_objective_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_plus, gamma_plus = gml.algorithm1_similar(M_original_eye,X_train_normalized,w,G_all,T_all,3)\n",
    "z_minus, gamma_minus = gml.algorithm1_dissimilar(M_original_eye,X_train_normalized,w, G_all, T_all, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = wasserstein.matrix_from_vector(gamma_plus,64)\n",
    "diff_zero = wasserstein.tril_vector(M_original_eye) - wasserstein.tril_vector(M_original)\n",
    "\n",
    "res1 = gamma_plus.T.dot(diff_zero)\n",
    "result = np.dot(gamma[].T, diff_zero)\n",
    "np.trace(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = np.trace(gamma_plus.dot(diff_zero))\n",
    "\n",
    "upper = grad_objective_plus[np.triu_indices(64,1)]\n",
    "upper.shape\n",
    "lower = np.tril(grad_objective_plus,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
